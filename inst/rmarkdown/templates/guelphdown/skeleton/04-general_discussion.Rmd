
```{r package_loading_int_conc, include=F}
#devtools::install_github(repo = 'sciarraseb/nonlinSimsAnalysis', force = T)
library(easypackages)
packages <- c('tidyverse', 'RColorBrewer', 'parallel', 'data.table', 'kableExtra', 'ggtext', 'egg', 'ggbrace', 'cowplot', 'nonlinSimsAnalysis', 'nonlinSims', 'knitr')
libraries(packages)

knitr::opts_chunk$set(message = F)
```


# General Discussion 

In systematically reviewing the simulation literature, I found that studies rarely conducted comprehensive investigations into the effects of longitudinal design and analysis factors on model performance with nonlinear patterns of change. Specifically, few studies examined three-way interactions between any of the following four variables: 1) measurement spacing, 2) number of measurements, 3) sample size, and 4) time structuredness. Given that longitudinal designs are necessary for understanding the temporal dynamics of psychological processes (for a more detailed explanation, see Appendix \ref{ergodicity}), it is important that researchers understand how longitudinal design and analysis factors affect the performance of longitudinal analyses. Therefore, to address these gaps in the literature, I designed three simulation experiments.

In each simulation experiment, a logistic pattern of change (i.e., s-shaped change pattern) was modelled under conditions that varied in nature of change (i.e., shape of the logistic curve), measurement number, sample size, and time structuredness.\footnote{Importantly, no simulation experiment manipulated more than three variables at once so that results would not be too difficult to understand \parencite{halford2005}.} To fit a logistic function where each parameter could be meaningfully interpreted, each simulation experiment used a structured latent growth model to estimate nonlinear change (for a detailed explanation, see Appendix \ref{structured-lgc}). 

To investigate the effects of longitudinal design and analysis factors on model performance, my simulation experiments examined the accuracy with which each logistic function parameter was estimated. In computing the estimation accuracy of each parameter, two questions were of importance: 1) How well was the parameter estimated on average (bias) and 2) what was a range of values that could be expected for an estimate from the output of a single model (precision). Thus, model performance was the combination of bias and precision, and these two metrics were computed for each logistic function parameter. To succinctly summarize each experiment, I have created Table \ref{tab:exp-summary-table}. Each row of Table \ref{tab:exp-summary-table} contains a summary of a simulation experiment.


In Experiment 1, I was interested in answering two questions: 1) Does placing measurements near periods of change increase model performance and 2) how should measurements be spaced when the nature of change is unknown. To answer these two questions, I manipulated measurement spacing, number of measurements, and nature of change (i.e., shape of the s-shaped curve). With respect to the first question, the results of Experiment 1 suggest that model performance increases when measurements are placed closer to periods of change (see section discussing [measurement spacing](#meas-placing)). With respect to the second question, the results of Experiment 1 suggest that measurements should be spaced equally over time when the nature of change is unknown (see section discussing measurement spacing when the nature of change is [unknown](#unknown)). 

In Experiment 2, I was interested in the measurement number/sample size pairings needed to obtain high model performance (i.e., low bias, high precision) under different spacing schedules. To answer this question, I manipulated measurement spacing, measurement number, and sample size. Although no manipulated measurement number/sample size pairing results in high model performance (low bias, high precision) of all parameters, moderate measurement numbers and sample sizes often yield low bias and the largest improvements in model performance. For all spacing schedules (except middle-and-extreme spacing), the largest improvements in model performance result from using either either seven measurements with *N* $\ge$ 200 or nine measurements with *N* $\le$ 100. The results for middle-and-extreme spacing are largely a byproduct of the nature of change used in Experiment 2, and so are of little value to emphasize. 


```{r exp-summary-table, echo=F}
summary_df <- data.frame('Simulation Exeriment' = c("Experiment 1", "Experiment 2", "Experiment 3"), 
                         'Independent Variables' = c("\\thead[lt]{Spacing of measurements \\\\ Number of measurements \\\\ Nature of change}", 
                                                     "\\thead[lt]{Spacing of measurements \\\\ Number of measurements \\\\ Sample size}", 
                                                     "\\thead[lt]{Number of Measurements \\\\ Sample size \\\\ Time structuredness}"), 
                         'Main Results' = c('\\tabitem Model performance is higher when measurements are placed closer to periods of change \\newline
                                            \\tabitem Measurements should be spaced equally when the nature of change is unknown',
                                            '\\tabitem The greatest improvements in model performance result from using either seven measurements with $N \\ge$ 200 or nine measurements with $N \\le$ 100',
                                            '\\tabitem The greatest improvements in model performance across all time structuredness levels result from using either seven measurements with $N \\ge$ 200 or nine measurements with $N \\le$ 100 \\newline
                                            \\tabitem Use definition variables to prevent model performance from decreasing as time structuredness decreases'), check.names = F)
kbl(x = summary_df, format = 'latex', digits = 2,align = c('l', 'l', 'l'), 
    longtable = T, booktabs = T,  escape = F,
    caption = 'Summary of Each Simulation Experiment') %>%
    column_spec(column = 3, width = '7.25cm') %>%
   kable_styling(latex_options= c('repeat_header'), position = 'left')
```


In Experiment 3, I was interested in examining how time structuredness affected model performance. To answer this question, I manipulated measurement spacing, measurement number, and time structuredness. Although the measurement number/sample size pairings that result in the greatest improvements in model performance are the same as in Experiment 2, two results suggest that model performance decreases as time structuredness decreases. First, precision worsens as time structuredness decreases. That is, precision worsens as response patterns of participants become increasingly dissimilar. Second, and more concerning, bias increases as time structuredness decreases regardless of the measurement number or sample size. That is, as response patterns of participants become increasingly dissimilar, bias increases across all measurement number/sample size pairings.

Importantly, the decrease in model performance that results as time structuredness decreases can be prevented by using a latent growth curve model with definition variables. By default. latent growth curve models assume an identical response pattern for all participants (i.e., time-structured data). Definition variables can be used in latent growth curve models to allow individual response patterns to be modelled [@mehta2000; @mehta2005]. In an additional set of simulations (see section on [definition variables](#def-variables)), I generate time-unstructured data and analyze the data with a structured latent growth curve model that has definition variables. When definition variables are used, the decrease in model performance that results from a decrease in time structuredness disappears. Therefore, to obtain the largest improvements in model performance, either seven measurements with *N* $\ge$ 200 or nine measurements with *N* $\le$ 100 must be used and, importantly, the latent growth curve model must use definition variables. 

In summary, the results of my simulation experiments are the first (to my knowledge) to provide specific measurement number and sample size recommendations needed to accurately model nonlinear change over time. Importantly, although previous studies have investigated the effects of some longitudinal design and analysis factors on model performance with nonlinear patterns of change, the results of these studies are limited because they either use unrealistic fixed-effects models [e.g., @finch2017], use models with  with non-meaningful parameter interpretations [e.g., @fine2019; @liu2022], or use unrealistic model fitting procedures [@finch2017]. Additionally, I developed novel and replicable procedures for creating spacing schedules (see Appendix \ref{measurement-schedules}) and simulating time-unstructured data (see [time structuredness](#simulating-time-struc)). 

The sections that follow will discuss the limitations of the current simulation experiments and avenues for future research. The scope of the discussion will then expand to include issues concerning the nature of longitudinal designs, the importance of modelling nonlinear change, and suggestions for modelling such change.

## Limitations and Future Directions 

Recall that in designing each simulation experiment, I decided to manipulate no more than three variables so that results could be readily understood [@halford2005]. Although limiting the number of independent variables has its advantages, there are a number of non-manipulated variables could have influenced the results. In the sections that follow, I review the possible impact of not manipulating these variables.  

### Cutoff Values for Bias and Precision

In simulation research, cutoff values for parameters are often set to a percentage of a parameter's population value [e.g., @muthen1997] for two reasons. First, cutoff values are needed to allow bias and precision to be categorized so that results can be clearly presented. In the current set of simulation experiments, cutoff values for bias and precision were set to 10% of the parameter's population value [@muthen1997]. If a parameter estimate was outside a 10% error margin, then estimation was considered biased. If an error bar whisker length was longer than 10% of the parameter's population value, then estimation was considered imprecise. Therefore, using cutoff values allows categorical decisions to be made modelling performance. 

Second, cutoff values are needed to allow results from different simulation studies to be meaningfully compared. If another study uses a cutoff value of 15%, then the results of this study become difficult to compare with the results of the current simulation experiments because each study uses different cutoff standards. Therefore, it is
important that simulation studies use a common standard of 10% [@muthen1997]---as I have done in my simulation
experiments. Although simulation studies use cutoff values to simplify results and allow meaningful comparisons of results, it is also important that cutoff values themselves represent meaningful boundary values.  

Given the need for using cutoff values in simulation research, it was necessary to do so in my experiments. Although several methods exist for setting cutoff values that each have their advantages and disadvantages, I decided to choose a method that aligned with the conventions of simulation research. Thus, I used a percentage-based cutoff rule [@muthen1997]. Like other methods for setting cutoff values, the percentage-based cutoff method has limitations and I discuss these limitations in the paragraphs that follow. 

In simply defining cutoff values as a percentage of a population value, cutoff values can lead to problematic conclusions. As a simple example, consider a scenario where a beverage company wants to produce a caffeinated drink that can only increase heart rate and body temperature by a certain amount. Specifically, neither heart rate nor body temperature can increase by 10% of their resting values. Given that, for males and females, any value below 70 and 80, respectively, constitutes a healthy resting heart rate [@nanchen2018], a 10% increase would translate to an increase of 7 and 8 beats per minute, which is arguably less than the increase in heart rate caused from walking [e.g., @whitley1987]. Thus, requiring that a caffeinated drink not increase resting heart rate by a value equal to or greater than 10% appears to be a responsible stipulation. Unfortunately, setting a 10%-cutoff rule for body temperature allows for far less desirable outcomes than a 10% cutoff for heart rate. Using a typical body temperature of 37 $^\circ$C for resting body temperature, a 10%-cutoff would allow for a change in body temperature of 3.7 $^\circ$C. Given that deviations of less than 3.7 $^\circ$C from resting body temperature can lead to physiological impairments and even death [@moran2002], restricting the caffeinated drink to not increase body temperature by 10% of its resting value is unwise. Therefore, a percentage cutoff rule can fail to create useful cutoff values by overlooking the underlying nature of the variable in question.

In the current simulation experiments, the percentage-cutoff rule may have led to overly pessimistic conclusions about model performance. As an example, consider the estimation of the random effect parameters. In each simulation experiment, no measurement number/sample size pairing resulted in high model performance (low bias, high precision) of any random-effect parameter.\footnote{It should be mentioned that low bias was obtained from using moderate measurement number/sample size pairings.} Specifically, the random-effect day-unit parameters were never modelled precisely with any measurement number/sample size pairing. Although the lack of precise estimation for the random-effect day-unit parameters is concerning, the result may be a byproduct of having used conventional standards for precision. For a given parameter, the cutoff value used to deem estimation precise was proportional to the population value set for that parameter. Specifically, the cutoff values for precision (and bias) were set to 10% of the parameter's population value [@muthen1997]---as is suggested by the literature. In setting the cutoff value to a percentage of the parameter's population value, the margin of error becomes a function of the population value: Large population values have large margins of error and small population values have small margins of error. Given that the random-effect parameters had the smallest population values (e.g., 10.00, 4.00, and 0.05) and that even the largest measurement number/sample size pairing of 11 measurements with *N* = 1000 did not result in good precision, it is conceivable that the associated 10%-error margins (e.g., 1.00, 0.04, and 0.005) may have been too small. 

Future research could consider using more useful cutoff values. One way to set useful cutoff values in simulation experiments is to contextualize cutoff values with respect to a real-world phenomenon. Using smallest effect sizes of interest offers one way to contextualize cutoff values [@lakens2017; @lakens2018]. Introduced to improve null-hypothesis significance testing, a smallest effect size of interest constitutes the smallest effect size above which a researcher considers an observed effect meaningful [@lakens2017]. Instead of testing the typical zero-effect null hypothesis, a researcher can specify a smallest effect size of interest as the null hypothesis. Using a smallest effect size of interest (in tandem with equivalence testing), a researcher can more definitively conclude whether an effect is trivially small or not and, consequently, be less likely to incorrectly dismiss an effect as nonexistent. Thus, smallest effect sizes of interest allow researchers to make more meaningful conclusions. Although the current simulation experiments did not employ significance testing, the cutoff values used to determine whether estimation was biased and precise could be improved in future research by treating them as smallest cutoff values of interest. By replacing the current percentage-based cutoff values with smallest cutoff values of interest for each parameter, conclusions are likely to become more meaningful because cutoff values are contextualized with respect to real-world phenomena. 

One effective way to determine smallest cutoff values of interest in future research would be to use anchor-based methods [@anvari2021]. As an example, I detail a two-step procedure for how an anchor-based method could be used to determine a cutoff value for a the Likert-unit parameter of the fixed-effect baseline parameter ($\uptheta_{fixed}$). First, a survey for some Likert-unit variable such as job satisfaction could be given at two time points to employees. Importantly, after completing the survey at the second time point, employees would also indicate how much job satisfaction changed by answering an anchor question (e.g., "Job satisfaction increased/decreased by a little, increased/decreased a lot, or did not change."). Second, a smallest cutoff value of interest would need to be computed. Given that the fixed-effect baseline parameter ($\uptheta_{fixed}$) represents the starting value, then employees that indicated no change in job satisfaction could be said to still be at baseline and their data could be used to compute a smallest effect size of interest for the baseline parameter($\uptheta_{fixed}$). Specifically, the difference in job satisfaction between the two time points could be calculated for employees that indicated no change. Therefore, using the anchor-based method, the smallest cutoff value of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) is the mean change in some Likert-unit variable---job satisfaction in the current example---from respondents that indicate no change.\footnote{If the mean observed change in job satisfaction from employees that indicate no change is a near-zero value, using this value as a smallest effect-size of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) would likely be too conservative. In such situations, the smallest effect-size of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) could be determined by computing the mean change in job satisfaction from employees that indicate a small change (i.e., "little increase/decrease"), as it could be said that these employees have slightly moved away from baseline.}


### External Validity of Simulation Experiments

In the current set of simulation experiments, data were were generated under ideal conditions in three ways. First, the current simulation experiments always assumed complete data (i.e., 100% response rate). Unfortunately, researchers rarely obtain complete data and, instead, have some amount of data that are missing. One investigation estimated that, using a sample of 300 articles published over a period of three years, 90% of articles had missing data, with each study estimated to have over 30% of data points missing [@mcknight2007, Chapter 1]. Perhaps even more concerning, missing data often compound over time [@newman2003].\footnote{It should be noted that great recommendations exist on increasing response rate. In fact, an entire book of recommendations exists on this issue \parencite[see][]{dillman2014}.} Future research could simulate more realistic conditions for response rates in longitudinal designs, missing data could be set to increase---either linearly or nonlinearly---over time under three types of commonly simulated missing data mechanisms: 1) missing data are random, 2) missing data depend on the value of another variable, and 3) missing data depend on their own values [@newman2009]. 


Second, the current simulation experiments assumed measurement invariance over time. That is, at each time point, the manifest variable was assumed to be measured with the same measurement model---specifically, aspects of the measurement model such as factor loadings, intercepts, and error variances were assumed to remain constant over time [@mellenbergh1989; @vandenberg2000]. For a longitudinal design, it is important that the measurement of a latent variable meet the conditions for invariance so that change over time can be meaningfully interpreted. As an example, consider a situation where a researcher measures some latent variable over time such as job satisfaction using a four-item survey where each item measures some component of job satisfaction on a Likert scale (range of 1--5). If the loadings of a specific item change over time, then the response values from participants cannot be meaningfully interpreted. For example, if a participant gives the same answers to each item across two time points but factor loadings of any item(s) change between the two time points, then their job satisfaction scores between the time points will, counterintuitively, be different. Thus, even though job satisfaction did not change over time, changes in the measurement model of job satisfaction caused the observed scores to be different. Unfortunately, measurement invariance is seldom observed [@vandenberg2000; @vandeschoot2015] because measurement model components often change over time [e.g., @fried2016]. Thus, it can be argued that it is more realistic to assume measurement non-invariance. To simulate measurement non-invariance, future research could generate data such that aspects of measurement models change over time [e.g., @kim2014a]. 

Third, the current simulations assumed error variances in the observed variables to be constant and uncorrelated over time. Unfortunately, error variances over time are likely to correlate with each other and be nonconstant or heterogeneous [@goldstein1994; @deshon1998; @bliese2002; @braun2013; @ding2016; @lester2019; @blozis2018]. Future research could simulate more realistic error variance structures by generating errors to correlate with each other and to decrease over time---as observed in a longitudinal analysis of fatigue [@lang2018]. 

### Simulations With Other Longitudinal Analyses

Given that researchers are often interested in investigating questions outside of modelling a nonlinear pattern of change, longitudinal analyses outside of the structured latent growth curve model used in the current simulation experiments may be used in other circumstances. Although the structured latent growth curve modelling framework used in the current simulations allows nonlinear change to be meaningfully modelled (see Appendix \ref{structured-lgc-code}), the framework cannot be used to understand all meaningful components of change. As an example, if a researcher is interested in modelling different response patterns in some variable in response to some organizational event---for instance, work engagement patterns after mergers [@seppälä2018]---a structured latent growth curve model could not meaningfully model such data because it assumes one pattern of responding. Therefore, to develop a comprehensive understanding of change over time, a variety of longitudinal analyses may be considered and it is important that future simulation research investigate the performance of these analyses. I outline four longitudinal analyses below that future simulation experiments should consider investigating. 

First, discontinuous growth models are needed to model punctuated change [@bliese2016; @bliese2020].\footnote{In the multilevel framework, discontinuous growth modelling is also referred to as piecewise hierarchical linear modelling \parencite{raudenbush2002} and multiphase mixed-effects models \parencite{cudeck2002}. In the latent variable or structural equation modelling framework, discontinuous growth modelling is also referred to as piecewise growth modelling \parencites{chou2004}{kohli2013}. Note that spline models are technically different from discontinuous growth models because spline models cannot model vertical displacements at knot points and, thus, are models for continuous change \parencite[for a review, see][]{edwards2017}.} Given that change in organizations often results from discrete events, the pattern of change is often punctuated or discontinuous [@morgeson2015]. Examples of punctuated change in organizations have been observed in life satisfaction after unemployment [@lucas2004], trust after betrayal [@fulmer2015], and firm performance after an economic recession [@kim2014b; for more examples, see @bliese2016]. Discontinuous growth models can model punctuated change by selectively activating and deactivating growth factors---that is, assigning nonzero- and zero-value weights, respectively---after certain time points [@bliese2016]. Therefore, given that punctuated change merits the need for discontinuous growth modelling in organizational research, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 


Second, time series models are needed to model cyclical patterns [@pickup2014]. Technological advances such as smartphones and wearable sensors have allowed researchers to collect intensive longitudinal data sets where data are collected over at least 20 time points [@collins2006] with the experience sampling method [@larson2014]. With intensive longitudinal data sets, researchers are often interested in modelling cyclical patterns such as those with affect and performance [@dalal2014] and stress [@fuller2003]. Time series models allow researchers to model cyclical patterns through a variety of methods (e.g., decomposition, autoregressive integrated moving average, etc.). Therefore, the rise of intensive longitudinal data made possible by technological advances merits the use of time series models, and future simulation studies should investigate the effects of longitudinal design and analysis factors affect the performance of such models. 

Third, second-order growth models are needed to model measurement invariance [@sayer2001; @hancock2001]. In organizational research, many variables are latent---that is, they cannot be directly observed (e.g., job satisfaction, organizational commitment, trust). Because latent variables cannot be directly measured, nomological networks\footnote{Although a nomological network gives meaning to a latent variable by specifying relations with other variables, it should be noted that nomological networks have limitations in establishing validity---whether a survey measures what is purports to measure. In psychology, almost all variables psychology are correlated with each other \parencite{meehl1978}, and so using the correlations specified in a nomological network to establish validity is imprecise because many latent variables are likely to satisfy the network of relations. One potentially more effective method to establish validity is to first assume the existence of the latent variable and then develop theory that specifies processes by which changes in the latent variable manifest themselves in reality. Surveys can the be constructed by causatively testing whether the theorized manifestations that follow from changes in the latent variable actually emerge \parencite[for a review, see][]{borsboom2004}.}---correlation matrices specifying relations between the target latent variable and other variables---are constructed to develop valid measures of latent variables [@cronbach1955]. As discussed previously, an unfortunate phenomenon with surveys is that the accuracy with which they measure a latent variable is seldom invariant over time---that is, measurement accuracy is often non-invariant [@vandenberg2000; @vandeschoot2015]. If measurement non-invariance is overlooked, model performance decreases [@kim2014b; @jeon2020]. Fortunately, second-order latent growth curve models allow researchers to include measurement models and, thus, test for measurement invariance and estimate parameters with greater accuracy [e.g., @kim2014a]. Therefore, given that the common occurrence of measurement non-invariance in organizational research merits the use of second-order latent growth models, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 

Fourth, growth mixture models are needed to model heterogeneous response patterns [@wang2007; @vandernest2020]. In organizations, employees are likely to respond to changes in different ways, thus exhibiting heterogeneous response patterns. Examples of heterogeneous response patterns have been observed in job performance patterns during organizational restructuring [@miraglia2015], work engagement patterns after mergers [@seppälä2018], and leadership development throughout training [@day2011]. Growth mixture models allow heterogeneity in response patterns to be modelled by including a latent categorical variable that allows participants to be placed into different response category patterns [cf. @bauer2007]. Therefore, given that heterogeneous response patterns in organizations merit the use of interest for modelling cyclical patterns with intensive longitudinal data merits the use of time series models, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 


## Nonlinear Patterns and Longitudinal Research
### A New Perpective on Longitudinal Designs for Modelling Change

The results of the current simulation experiments suggest that previous measurement number recommendations for longitudinal research need to be modified when modelling nonlinear patterns of change. Previous suggestions for conducting longitudinal research recommend that at least three measurements be used [@chan1998;@ployhart2010]. The requirement that a longitudinal study use at least three measurements is largely to obtain an estimate of change that is not confounded by measurement error [@rogosa1982] and allow a nonlinear pattern of change to be modelled. Unfortunately, although using at least three measurements allows a nonlinear pattern of change to be modelled, doing so provides no guarantee that a nonlinear pattern of change will be accurately modelled. The results of the current simulation experiments suggest that, at the very least, five measurements are needed to accurately model a nonlinear pattern of change. Importantly, five measurements only results in adequate model performance if the measurements are placed near periods of change. Given that organizational theories seldom delineate nonlinear patterns of change [for a rare example, see @methot2017], it is unlikely that researchers will place measurements near periods of change. In situations where researchers have little insight into the pattern of nonlinear change, the current simulation experiments suggest that at least seven measurements be used. Therefore, when researchers do not have strong theory to suggest a nonlinear pattern of change, the current simulations suggest that at least seven measurements are needed. 

Although the current results suggest that seven measurements are needed to model nonlinear change, these results by no means imply that longitudinal designs with fewer measurements are of no value. Studies measuring a variable at two time points (i.e., pre-post designs) can be used to estimate meaningful anchors [@anvari2021]. Studies measuring change between three and seven time points can, for instance, be used to investigate causality by determining whether reverse causality occurs [@leszczensky2019]. As a last point, it should be noted that studies using fewer than seven measurements may be able to provide accurate parameter estimates for nonlinear models that estimate fewer parameters than the nine parameters estimated by the model in the current simulations. If a latent variable model estimates fewer parameters, the optimization problem becomes less complex, and so it is conceivable that the convergence algorithm can find accurate parameter estimates with fewer than seven measurements. 


### Why is it Important to Model Nonlinear Patterns of Change?

For at least 30 years, research in organizational psychology has had a minimal effect on practitioners and their practices [@daft1990; for a review, see @lawler2022]. Few practitioners--specifically, an estimated 1%---read journal articles [@rynes2002a], which is accompanied by a poor understanding by managers of fundamental principles in organizational psychology, which has been observed across multiple countries including the Netherlands [@sanders2008], the United States [@rynes2002a], Finland, South Korea, and Spain [@tenhiälä2014]. Perhaps most unfortunate, a poor understanding of organizational psychology by managers is associated with large effects on financial and individual performance [for a review, see @rynes2002b]. Additionally, an estimated 55% of practitioners are skeptical that evidence-based human resource practices can effect any positive change [@kpmg2015]. With the gap between academics and practitioners being so patently wide, some academics have cast doubt on the possibility of academic-practitioner research collaborations [@kieser2009].  

One factor that may contribute to the academic-practitioner gap is that research seldom provides specific recommendations to practitioners. When considering the typical organizational theory, propositions often lack any degree of specificity: They often specify non-zero linear relations between variables [@edwards2010]. Because it is difficult to develop specific recommendations from non-zero relations, it becomes unsurprising that reviews of the organizational literature estimate 3% of human resource articles address problems faced by practitioners [@sackett1990] and, in reviewing of 5780 articles from 1963--2007, concluded that research is often late to address practitioner issues [@cascio2008]. Thus, with organizational theories often providing vague predictions, it becomes difficult to develop specific recommendations for practitioners. 

Organizational research can provide specific recommendations to practitioners by modelling nonlinear patterns of change. In modelling nonlinear change, organizational researcher can understand how processes unfold over time and when specific psychological phenomena emerge [@mitchell2001; @navarro2020]. As an example of the usefulness of modelling nonlinear change, @vancouver2020 uses computational modelling to predict specific nonlinear patterns of self-efficacy and performance in response to different events over time. In predicting nonlinear patterns, the theory provides specific insight into how much specific events affect performance and self-efficacy, how long such effects last, and how performance and self-efficacy affect each other. Given that change over time is likely to be nonlinear [@cudeck2007], it is likely that many opportunities exist for organizational research to provide specific recommendations for solving problems faced by practitioners. 

In summary, a concerning gap exists between academics and practitioners in organizational research whereby academics seldom address the problems faced by practitioners [e.g., @sackett1990] and practitioners rarely consult research when making decisions [@rynes2002b]. One cause for the academic-practitioner gap is the paucity of specific recommendations provided by academics. One way that academics can reduce the gap from practitioners is to model nonlinear patterns of change over time. In modelling a nonlinear patterns of change, organizational research can develop an understanding o how processes evolve over time and when psychological phenomena emerge [@mitchell2001; @navarro2020]. With an understanding of the temporal dynamics of psychological processes, organizational research can then provide specific recommendations to practitioners. 


### Suggestions for Modelling Nonlinear Change

In modelling nonlinear change, researchers can either do so using the multilevel or latent growth curve framework. Although the multilevel and latent growth curve frameworks return identical results under many conditions [e.g., @bauer2003], researchers should consider using the latent growth curve framework over the multilevel framework for two reasons. First, the multilevel framework encounters convergence problems when specifying nonlinear models, and the frequency of convergence problems increases with the number of random-effect parameters [for a review, see @mcneish2020]. Second, the latent variable framework allows data to be more realistically modelled than the multilevel approach thanks to, in large part, its ability to include measurement models to investigate phenomena such as measurement invariance [@sayer2001; @hancock2001].

In modelling nonlinear change, researchers should prioritize the interpretability of their models so that results can be more easily applied. As an example, the structured latent growth curve model used in the current simulation experiments provides a meaningful representation of logistic pattern of change. In the current simulations, the number of days needed to reach the halfway- and triquarter-halfway elevation points (among other parameters) were estimated.\footnote{Note that parameters of nonlinear functions can be reparameterized to estimate other meaningful aspects of a curve \parencite{preacher2015}.} To add another level of meaning, a latent categorical variable can be added to the model to create a growth mixture model [@vandernest2020]. Using a growth mixture model, not only can nonlinear change be defined in a meaningful way, but response groups can be modelled and people can be categorized into the groups based on their individual pattern of change. Thus, in prioritizing the meaning of statistical models, the current example shows how heterogeneous logistic response patterns can be meaningfully modelled and how frequently each pattern occurs. 


## Conclusion 

Investigating nonlinear patterns of change is a growing area of organizational research. By understanding nonlinear patterns of change, organizational research can develop a more nuanced understanding of temporal dynamics and provide practitioners with more specific recommendations. The simulation experiments conducted in my dissertation contribute to this goal by providing boundary conditions for model performance.  



