
`
```{r code-block-numbering, include=F}
library(easypackages)
packages <- c('knitr')
libraries(packages)

knitr::opts_chunk$set(message = F)

#see https://stackoverflow.com/questions/50702942/does-rmarkdown-allow-captions-and-references-for-code-chunks 
#note that \\restoreparindent is needed so that \\captionof does not eliminate paragraph indenting
oldSource <- knit_hooks$get("source")
knit_hooks$set(source = function(x, options) {
  x <- oldSource(x, options)
  x <- ifelse(!is.null(options$ref), paste0("\\label{", options$ref,"}", x), x)
  ifelse(!is.null(options$codecap), paste0("\\captionof{chunk}{", options$codecap,"}", "\\restoreparindent", x), x)
})

```

```{=tex}
%change numbering for figures, tables, and equations for appendices
\renewcommand\thefigure{\theapp.\arabic{figure}} %change figure numbering for appendix such that it goes A.1, A.2, etc.
\counterwithin{figure}{app} %reset figure number counter for each appendix

\renewcommand\thetable{\theapp.\arabic{table}} %change figure numbering for appendix such that it goes A.1, A.2, etc.
\counterwithin{table}{app} %reset figure number counter for each appendix

%reset equation number number counter for each appendix
\renewcommand{\theequation}{\theapp.\arabic{equation}}
\counterwithin{equation}{app} %reset figure number counter for each appendix

\counterwithin{chunk}{app} %reset code chunk numbering
```

\app{Ergodicity and the Need to Conduct Longitudinal Research}
\label{ergodicity}

To understand why cross-sectional results are unlikely to agree with longitudinal results for any given analysis, a discussion of data structures is apropos. Consider an example where a researcher obtains data from 50 people measured over 100 time points such that each row contains a $p$ person's data over the 100 time points and each column contains data from 50 people at a $t$ time point. For didactic purposes, all data are assumed to be sampled from a normal distribution. To understand whether findings in any given cross-sectional data set yield the same findings in any given longitudinal data set, the researcher randomly samples one cross-sectional and one longitudinal data set and computes the mean and variance in each set. To conduct a cross-sectional analysis, the researcher randomly samples the data across the 50 people at a given time point and computes a mean of the scores at the sampled time point ($\bar{X}_t$) using Equation \ref{eq:cross-mean} shown below:

\begin{align}
\bar{X}_t = \frac{1}{P}\sum^P_{p = 1} x_p,
(\#eq:cross-mean)
\end{align}

\subapp{Example} 

```{example, taylor-estimates, echo=T}
Estimates of Taylor series approximation of $f(x) = \cos(x)$ as the difference between the point of evaluation $\mathrm{x}$ and the point of derivation $\mathrm{a}$ increases.

\textup{Taylor series approximation of $\cos(x)$ (specifically, the second-order Taylor series; $P^2[\cos(x), a]$) estimates values that are exactly equal to the values returned by $\cos(x)$ when the point of evaluation (\textit{x}) is set to the point of derivation (\textit{a}). The example below computes the value predicted by the Taylor series approximation of $P^2[\cos(x), a]$ and by $\cos(x)$ when \textit{x} = \textit{a} = 0.}

\begin{align*}
P^2(\cos(x=0), a=0) &= \cos(x=0) \nonumber \\ 
1- \frac{1}{2}x^2 &=  \cos(0) \nonumber \\ 
1- \frac{1}{2}0^2 &=  1 \nonumber \\ 
1- 0 &=  1 \nonumber \\ 
1 &=  1 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

\textup{Taylor series approximation of $\cos(x)$ (specifically, the second-order Taylor series; $P^2[\cos(x), a]$) estimates a value that is approximately equal ($\thickapprox$) to the value returned by $f\cos(x)$ when the difference between the point of evaluation \textit{x} and the point of derivation \textit{a} is small. The example below computes the value predicted by the Taylor series approximation of $P^2[\cos(x), a]$ and by $\cos(x)$ when \textit{x} = 1 and  \textit{a} = 0.} 

\begin{align*}
P^2(\cos(x = 1), 0) &\thickapprox \cos(x = 1) \nonumber \\ 
1- \frac{1}{2}x^2 &\thickapprox   \cos(1) \nonumber \\ 
1- \frac{1}{2}1^2 &\thickapprox   0.54 \nonumber \\ 
1- 0.5 &\thickapprox   0.54 \nonumber \\ 
0.5 &\thickapprox 0.54 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

\textup{Taylor series approximation of $f\cos(x)$ (specifically, the second-order Taylor series; $P^2[\cos(x), a]$) estimates a a value that is clearly not equal ($\neq$) to the value returned by $f\cos(x)$ when the difference between the point of evaluation \textit{x} and the point of derivation \textit{a} is large. The example below computes the value predicted by the Taylor series approximation of $P^2[\cos(x), a]$ and by $\cos(x)$ when \textit{x} = 4 and  \textit{a} = 0.} 


\begin{align*}
P^2(\cos(x = 4), 0) &\neq \cos(x = 4) \nonumber \\ 
1- \frac{1}{2}x^2 &\neq  \cos(4) \nonumber \\ 
1- \frac{1}{2}4^2 &\neq  -0.65 \nonumber \\ 
1- 16 &\neq  -0.65 \nonumber \\ 
0.5 &\neq  -0.65 \nonumber \\ 
\end{align*}
\vspace*{-25mm}

\noindent \hrulefill
```

\app{Code block}

The code that I used to model logistic pattern of change (see [data generation](#data-generation)) is shown in Code Block \ref{structured-model}. Note that, the code is largely excerpted from the `run_exp_simulations()` and `create_logistic_model_ns()` functions from the `nonlinSims` package, and so readers interested in obtaining more information should consult the source code of this package. One important point to mention is that the model specified in Code Block \ref{structured-model} assumes time-structured data. 

```{r structured-lgc, echo=T, eval=F, attr.source='.numberLines', ref = "structured-model", codecap = "OpenMx Code for Structured Latent Growth Curve Model That Assumes Time-Structured Data"}
#Days on which measurements are assumed to be taken (note that model assumes time-structured data; that is, at each time point, participants provide data at the exact same moment). The measurement days obtained by finding the unique values in the `measurement_day` column of the generated data set. 
measurement_days <- unique(data$measurement_day) 

#Manifest variable names (i.e., names of columns containing data at each time point,
manifest_vars <- nonlinSims:::extract_manifest_var_names(data_wide = data_wide)

#Now convert data to wide format (needed for OpenMx)
data_wide <- data[ , c(1:3, 5)] %>% 
    pivot_wider(names_from = measurement_day, values_from = c(obs_score, actual_measurement_day))
  
#Remove . from column names so that OpenMx does not run into error (this occurs because, with some spacing schedules, measurement days are not integer values.) 
names(data_wide) <- str_replace(string = names(data_wide), pattern = '\\.', replacement = '_')

#Latent variable names (theta = baseline, alpha = maximal elevation, beta = days-to-halfway elevation, gamma = triquarter-haflway elevation)
latent_vars <- c('theta', 'alpha', 'beta', 'gamma') 

latent_growth_curve_model <- mxModel(
  model = model_name,
  type = 'RAM', independent = T,
  mxData(observed = data_wide, type = 'raw'),
  
  manifestVars = manifest_vars,
  latentVars = latent_vars,
  
  #Residual variances; by using one label, they are assumed to all be equal (homogeneity of variance). That is, there is no complex error structure. 
  mxPath(from = manifest_vars,
         arrows=2, free=TRUE,  labels='epsilon', values = 1, lbound = 0),
  
  #Latent variable covariances and variances (note that only the variances are estimated. )
  mxPath(from = latent_vars,
         connect='unique.pairs', arrows=2,
         free = c(TRUE,FALSE, FALSE, FALSE, 
                  TRUE, FALSE, FALSE, 
                  TRUE, FALSE, 
                  TRUE), 
         values=c(1, NA, NA, NA, 
                  1, NA, NA, 
                  1, NA,
                  1),
         labels=c('theta_rand', 'NA(cov_theta_alpha)', 'NA(cov_theta_beta)', 
                  'NA(cov_theta_gamma)',
                  'alpha_rand','NA(cov_alpha_beta)', 'NA(cov_alpha_gamma)', 
                  'beta_rand', 'NA(cov_beta_gamma)', 
                  'gamma_rand'), 
         lbound = c(1e-3, NA, NA, NA, 
                    1e-3, NA, NA, 
                    1, NA,
                    1), 
         ubound = c(2, NA, NA, NA, 
                    2, NA, NA, 
                    90^2, NA, 
                    45^2)),
  
  # Latent variable means (linear parameters). Note that the parameters of beta and gamma do not have estimated means because they are nonlinear parameters (i.e., the logistic function's first-order partial derivative with respect to each of those two parameters contains those two parameters. )
  mxPath(from = 'one', to = c('theta', 'alpha'), free = c(TRUE, TRUE), arrows = 1,
         labels = c('theta_fixed', 'alpha_fixed'), lbound = 0, ubound = 7, 
         values = c(1, 1)),
  
  #Functional constraints (needed to estimate mean values of fixed-effect parameters)
  mxMatrix(type = 'Full', nrow = length(manifest_vars), ncol = 1, free = TRUE, 
           labels = 'theta_fixed', name = 't', values = 1, lbound = 0,  ubound = 7), 
  mxMatrix(type = 'Full', nrow = length(manifest_vars), ncol = 1, free = TRUE, 
           labels = 'alpha_fixed', name = 'a', values = 1, lbound = 0,  ubound = 7), 
  mxMatrix(type = 'Full', nrow = length(manifest_vars), ncol = 1, free = TRUE, 
           labels = 'beta_fixed', name = 'b', values = 1, lbound = 1, ubound = 360),
  mxMatrix(type = 'Full', nrow = length(manifest_vars), ncol = 1, free = TRUE, 
           labels = 'gamma_fixed', name = 'g', values = 1, lbound = 1, ubound = 360), 

  mxMatrix(type = 'Full', nrow = length(manifest_vars), ncol = 1, free = FALSE, 
           values = measurement_days, name = 'time'),
  
  #Algebra specifying first-order partial derivatives; 
  mxAlgebra(expression = 1 - 1/(1 + exp((b - time)/g)), name="Tl"),
  mxAlgebra(expression = 1/(1 + exp((b - time)/g)), name = 'Al'), 
  
  mxAlgebra(expression = -((a - t) * (exp((b - time)/g) * (1/g))/(1 + exp((b - time)/g))^2), name = 'Bl'),
  mxAlgebra(expression =  (a - t) * (exp((b - time)/g) * ((b - time)/g^2))/(1 + exp((b -time)/g))^2, name = 'Gl'),
  
  #Factor loadings; all fixed and, importantly, constrained to change according to their partial derivatives (i.e., nonlinear functions) 
  mxPath(from = 'theta', to = manifest_vars, arrows=1, free=FALSE,  
         labels = sprintf(fmt = 'Tl[%d,1]', 1:length(manifest_vars))),
  mxPath(from = 'alpha', to = manifest_vars, arrows=1, free=FALSE,  
         labels = sprintf(fmt = 'Al[%d,1]', 1:length(manifest_vars))), 
  mxPath(from='beta', to = manifest_vars, arrows=1,  free=FALSE,
         labels =  sprintf(fmt = 'Bl[%d,1]', 1:length(manifest_vars))), 
  mxPath(from='gamma', to = manifest_vars, arrows=1,  free=FALSE,
         labels =  sprintf(fmt = 'Gl[%d,1]', 1:length(manifest_vars))), 
  
  #Fit function used to estimate free parameter values. 
  mxFitFunctionML(vector = FALSE)
)

#Use starting value function from OpenMx to generate good starting values (uses weighted least squares)
latent_growth_model <- mxAutoStart(model = latent_growth_model)

#Fit model using mxTryHard(). Increases probability of convergence by attempting model convergence by randomly shifting starting values. 
model_results <- mxTryHard(latent_growth_model)
  
```







